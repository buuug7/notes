# 字符、编码和 Java 中的编码

作者：刘惜有
链接：https://www.jianshu.com/p/1b00ca07b003
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

字符是用户可以读写的最小单位。计算机所能支持的字符组成的集合，就叫做字符集。字符集通常以二维表的形式存在。二维表的内容和大小是由使用者的语言而定，是英语、是汉语、还是阿拉伯语。人类阅读的文章是由字符组成的，而计算机是通过二进制字节进行信息传输的。计算机无法直接传输字符，所以就需要将字符解析成字节，这个解析操作就叫做编码（encode），而相应的，将编码的字节还原成字符的操作就叫做解码（decode）。编码和解码都需要按照一定的规则，这种把字符集中的字符编码为特定的二进制数的规则就是字符编码（Character encoding）。网上也有称之为字符集编码的，其实大概就是一个意思，注意和后面的编码字符集区分。

## 为什么要编码

由于人类的语言有太多，因而表示这些语言的符号太多，无法用计算机中一个基本的存储单元——字节 来表示，因而必须要经过拆分或一些翻译工作，才能让计算机能理解。计算机中存储信息的最小单元是一个字节即 8 个位（bit），所以能表示的字符范围是 0~255 个。人类要表示的符号太多，无法用一个字节来完全表示，这就需要编码来解决这个问题。

## 早期字符编码发展

在计算机发展的早期，字符集和字符编码一般使用相同的命名，例如最早的字符集 ASCII（American Standard Code for Information Interchange），它既代表了计算机所支持显示的所有字符（字符集），又代表了这个字符集的字符编码。ASCII 字符集是一个二维表，支持 128 个字符。128 个码位，用 7 位二进制数表示，由于计算机 1 个字节是 8 位二进制数，所以最高位为 0，即 00000000-01111111 或 0x00-0x7F。ASCII（1963 年）和 EBCDIC（1964 年）这样的字符集逐渐成为计算机字符编码的早期标准。但这些字符集的局限很快就变得明显，于是人们开发了许多方法来扩展它们。

最初的拓展很简单，只是在原来 ASCII 的基础上，扩展为 256 个字符，成为 EASCII（Extended ASCII）。EASCII 有 256 个码位，用 8 位二进制数表示，即 00000000`11111111`或`0x00`0xFF。

当计算机传到了欧洲，EASCII 也开始不能满足需求了，但是改改还能凑合。于是国际标准化组织在 ASCII 的基础上进行了扩展，形成了 ISO-8859 标准，跟 EASCII 类似，兼容 ASCII，在高 128 个码位上有所区别。但是由于欧洲的语言环境十分复杂，所以根据各地区的语言又形成了很多子标准，如 ISO-8859-1、ISO-8859-2、ISO-8859-3、……、ISO-8859-16。

后来计算机传入亚洲，由于亚洲语种和文字都十分丰富，256 个码位就显得十分鸡肋了。于是继续扩大二维表，单字节改双字节，16 位二进制数，65536 个码位。在不同国家和地区又出现了很多编码，大陆的 GB2312、港台的 BIG5、日本的 Shift JIS 等等。

由于计算机的编码规范和标准在最初制定时没有意识到这将会是以后全球普适的准则，所以出现了各种各样的编码方式（也有了多种不同的字符集）。但是很多传统的编码方式都有一个共同的问题，即容许电脑处理双语环境（通常使用拉丁字母以及其本地语言），但却无法同时支持多语言环境（指可同时处理多种语言混合的情况）。而且不同国家和地区采用的字符集不一致，很可能出现无法正常显示所有字符的情况。

所以对于支持包括东亚 CJK 字符家族在内的写作系统的要求能支持更大量的字符，需要一种系统而不是临时的方法实现这些字符的编码。

## Unicode 字符编码五层次模型

为了解决传统的字符编码方案的局限，就引入了统一码（Unicode）和通用字符集（Universal Character Set, UCS）来替代原先基于语言的系统。通用字符集的目的是为了能够涵盖世界上所有的字符。由统一码（Unicode）和通用字符集（Universal Character Set, UCS）所构成的现代字符编码模型没有跟从简单字符集的观点。它们将字符编码的概念分为：有哪些字符、它们的编号、这些编号如何编码成一系列的代码单元，以及最后这些单元如何组成八位字节流。区分这些概念的核心思想是建立一个能够用不同方法来编码的一个通用字符集。

为了正确地表示 Unicode 和通用字符集模型，需要更多比“字符集”和“字符编码”更为精确的术语表示。在 Unicode Technical Report (UTR) #17 中，现代编码模型分为 5 个层次。

## 抽象字符表（Abstract character repertoire）

抽象字符表是操作系统支持的所有抽象字符的集合，决定了计算机能够展现表示的所有字符的范围。字符表中的字符可以决定如何划分输入的信息流。例如将一段中文划分成文字、标点、空格等，它们都能按照一种简单的线性序列排列。值得一提的是，对它们的处理需要另外的规则，如带有变音符号的字母这样的特定序列如何解释。为了方便起见，这样的字符表可以包括预先编号的字母和变音符号的组合。其它的书写系统，如阿拉伯语和希伯莱语，由于要适应双向文字和在不同情形下按照不同方式交叉在一起的字形，就使用更为复杂的符号表表示。

字符表可以是封闭的，即除非创建一个新的标准（ASCII 和多数 ISO/IEC 8859 系列都是这样的例子），否则不允许添加新的符号；字符表也可以是开放的，即允许添加新的符号（统一码和一定程度上代码页是这方面的例子）。

## 编码字符集（CCS:Coded Character Set）

编码字符集是将字符集 C 中每个字符映射到 1 个坐标（整数值对：x, y）或者表示为 1 个非负整数 N，C 中的每个字符对应的坐标或非负整数就称为码位（code position，也称码点：code point）。在 ASCII 中（这里的 ASCII 仅指编码字符集）的字符 A 对应一个数字 65，这个数字就是 A 的码位；在 GB 2312 中万字在 45 区 82 位，所以其码位是 45 82。码位其实也就是编码空间中的一个位置（position）。一个字符所占用的码位称为码位值（code point value）。

由于 GB 2312 用区和位来表示字符，因此也称为区位码。

1 个编码字符集就是把抽象字符映射为码位值，编码字符集就是字符集和码位的映射，也是一个元素为映射的集合。多个编码字符集可以表示同样的字符表，例如 ISO-8859-1 和 IBM 的代码页 037 和代码页 500 含盖同样的字符表但是将字符映射为不同的整数。这样就产生了编码空间（encoding space）的概念。

所谓的编码空间，就是包含所有字符的表的维度。如果字符集的字符映射到一个非负整数 N，如 ISO-8859-1 字符集中有 256 个字符，那就需要 256 个数字，每个字符对应一个数字，这所有的 256 个数字就构成了编码空间 (Code space)，即编码空间为 256（256 个码位）。如果字符集中的字符映射到一个坐标，那就用一对整数来描述，如 GB 2312 的汉字编码空间是 94 x 94。

编码空间也可以用字符的存储单元尺寸来描述，例如：ISO-8859-1 是一个 8 比特的编码空间。编码空间还可以用其子集来表述，如行、列、面（plane）等。

## 字符编码表（CEF:Character Encoding Form）

字符编码表也称为"storage format"，是将编码字符集的码位转换成码元（code units，也称“代码单元”）的序列。

码元指一个已编码的文本中具有最短的位（bit）组合的单元，是一个有限位长的整型值。对于 UTF-8 来说，码元是 8 位长；对于 UTF-16 来说，码元是 16 位长；对于 UTF-32 来说，码元是 32 位长[1]。码值（Code Value）是过时的用法。

定长编码的字符编码表是码位到自身的映射（null mapping），但变长编码中有些码位只映射到一个码元，而另一些码位会映射到多个码元（即由多个码元组成的序列）。例如，使用 16 位长的存储单元保存数字信息，系统每个单元只能够直接表示从 0 到 65,535 的数值，但是如果使用多个 16 位单元就能够表示更大的整数。这种映射可以使得在位长不变的情况下映射无限多的码元，这就是 CEF 的作用。

最简单的字符编码表就是单纯地选择足够大的单位，以保证编码字符集中的所有数值能够直接编码（一个码位对应一个码值）。这对于能够用使用八位元组来表示的编码字符集（如多数传统的非 CJK 的字符集编码）是合理的，对于能够使用十六位元来表示的编码字符集（如早期版本的 Unicode）来说也足够合理。但是，随着编码字符集的大小增加（例如，现在的 Unicode 的字符集至少需要 21 位才能全部表示），这种直接表示法变得越来越没有效率，并且很难让现有计算机系统适应更大的码值。因此，许多使用新近版本 Unicode 的系统，或者将 Unicode 码位对应为可变长度的 8 位字节序列的 UTF-8，或者将码位对应为可变长度的 16 位序列的 UTF-16。

## 字符编码方案（CES:Character Encoding Scheme）

字符编码方案也称作"serialization format"。它将定长的整型值（即码元）映射到 8 位字节序列，以便编码后的数据的文件存储或网络传输。

Unicode 仅使用一个简单的字符来指定字节顺序是大端序或者小端序（但对于 UTF-8 来说并不需要专门指明字节序）。有些复杂的字符编码机制（如 ISO/IEC 2022）会使用控制字符转义序列在几种编码字符集或者压缩机制之间切换。压缩机制用于减小每个单元所用字节数，常见的压缩机制有 SCSU、BOCU 和 Punycode。

## 传输编码语法（transfer encoding syntax）

传输编码语法用于处理上一层次的字符编码方案提供的字节序列。一般其功能包括两种：一是把字节序列的值映射到一套更受限制的值域内，以满足传输环境的限制，例如 Email 传输时 Base64 或者 quoted-printable，都是把 8 位的字节编码为 7 位长的数据；另一是压缩字节序列的值，如 LZW 或者行程长度编码等无损压缩技术。

## 五层模型与传统编码的术语比较

历史上的术语字符编码（character encoding），字符映射（character map），字符集（character set）或者代码页往往是同义概念，即字符表（repertoire）中的字符如何编码为码元的流（stream of code units）。通常每个字符对应单个码元。

所以例如 ASCII 这个名称，可能表示抽象字符集（ACR）、编码字符集（CCS）、字符编码表（CEF）、字符编码方案（CES）的任意一个或多个层次，在传统编码概念中 ASCII 这个名称也本身就代表一种字符编码，或者是一种字符集。具体表示什么要看上下文语义。平常我们所说的编码都在第三步的时候完成了,都没有涉及到 CES。

代码页（Codepage）通常意味着面向字节的编码，但强调是一套用于不能语言的编码方案的集合。著名的如"Windows"代码页系列，"IBM"/"DOS"代码页系列。

字符映射（character map）在 Unicode 中保持了其传统意义：从字符序列到编码后的字节序列的映射，包括了上述的 CCS, CEF, CES 层次。

高层机制（higher level protocol）提供了额外信息，用于选择 Unicode 字符的特定变种，如 XML 属性 xml:lang

由于不同国家和地区采用的字符集不一致，很可能出现无法正常显示所有字符的情况。微软公司使用了代码页转换表的技术来过渡性的部分解决这一问题，即通过指定的转换表将非 Unicode 的字符编码转换为同一字符对应的系统内部使用的 Unicode 编码。

Unix 或 Linux 不使用代码页概念，它们用 charmap，比 locales 具有更广泛的含义。

与上文的编码字符集（Coded Character Set - CCS）不同，字符编码（character encoding）是从抽象字符到代码字（code word）的映射。

HTTP（与 MIME）的用法中，字符集（character set）与字符编码同义，但与 CCS 不是一个意思。

## Unicode

Unicode（万国码、国际码、统一码、单一码）是计算机科学领域里的一项业界标准。它并不是一种具体的字符编码，而是对世界上大部分的文字系统进行了整理、编码，使得电脑可以用更为简单的方式来呈现和处理文字。

Unicode 伴随着通用字符集的标准而发展，至今仍在不断增修，每个新版本都加入更多新的字符。Unicode 编码包含了不同写法的字，如“ɑ／a”、“強／强”、“戶／户／戸”。可以简单地把通用字符集理解为五层模型中的抽象字符集（ACR），而 Unicode 就是字符编码表（CCS）。—— Wikipadia: Unicode

## 编码方式

Unicode 使用 16 位的编码空间也就是每个字符占用 2 个字节。这样理论上一共最多可以表示 2 的 16 次方（即 65536）个字符。基本满足各种语言的使用。实际上当前版本的统一码并未完全使用这 16 位编码，而是保留了大量空间以作为特殊使用或将来扩展。

随着 Unicode 的拓展，又增加了 16 个这样的平面。一开始的平面就称为基本多文种平面，也称 0 号平面，剩余的叫做辅助平面。两者合起来至少需要占据 21 位的编码空间，比 3 字节略少。Unicode 将 0 到 140 万的编码空间范围的每个码位映射到单个或多个在 0 到 655356 范围内的码元。

事实上辅助平面字符仍然占用 4 字节编码空间，与 UCS-4 保持一致。未来版本会扩充到 ISO 10646-1 实现级别 3，即涵盖 UCS-4 的所有字符。UCS-4 是一个更大的尚未填充完全的 31 位字符集，加上恒为 0 的首位，共需占据 32 位，即 4 字节。理论上最多能表示 231 个字符，完全可以涵盖一切语言所用的符号。

更详细请参考 Unicode 字符平面映射

## 实现方式

Unicode 的实现方式不同于编码方式。一个字符的 Unicode 编码是确定的。但是在实际传输过程中，由于不同系统平台的设计不一定一致，以及出于节省空间的目的，对 Unicode 编码的实现方式有所不同。

Unicode 的实现方式也称为 Unicode 转换格式（Unicode Transformation Format，简称为 UTF），目前主流的实现方式有 UTF-16 和 UTF-8。随着 Unicode 通用字符集的扩充，进而出现了 UTF-32，但是由于占用空间太大，目前很少有系统选择使用 utf-32 作为系统编码。

下面简单介绍 UTF-8 和 UTF-16 的实现。

## UTF-8

如果一个仅包含基本 7 位 ASCII 字符的 Unicode 文件，如果每个字符都使用 2 字节的原 Unicode 编码传输，其第一字节的 8 位始终为 0，这就造成了比较大的浪费。对于这种情况，可以使用 UTF-8 编码，这是一种变长编码，它将基本 7 位 ASCII 字符仍用 7 位编码表示，占用一个字节（首位补 0）。而遇到与其他 Unicode 字符混合的情况，将按一定算法转换。

UTF-8 每个字符使用 1-3 个字节编码，并利用首位为 0 或 1 进行识别。

对于 UTF-8 编码中的任意字节 B：

如果 B 的第一位为 0，那么代表当前字符为单字节字符，占用一个字节的空间。0 之后的所有部分（7 个位）代表在 Unicode 中的序号。
如果 B 以 110 开头，那么代表当前字符为双字节字符，占用 2 个字节的空间。110 之后的所有部分（5 个位）加上后一个字节的除 10 外的部分（6 个位）代表在 Unicode 中的序号。且第二个字节以 10 开头
如果 B 以 1110 开头，那么代表当前字符为三字节字符，占用 3 个字节的空间。1110 之后的所有部分（4 个位）加上后两个字节的除 10 外的部分（12 个位）代表在 Unicode 中的序号。且第二、第三个字节以 10 开头
如果 B 以 11110 开头，那么代表当前字符为四字节字符，占用 4 个字节的空间。11110 之后的所有部分（3 个位）加上后两个字节的除 10 外的部分（18 个位）代表在 Unicode 中的序号。且第二、第三、第四个字节以 10 开头
如果 B 以 10 开头，则 B 为一个多字节字符中的其中一个字节(非 ASCII 字符)
如下表：

| 码位的位数 | 码位起值  | 字节序列   | Byte1     | Byte2     | Byte3     | Byte4     | Byte5     | Byte6     |
| ---------- | --------- | ---------- | --------- | --------- | --------- | --------- | --------- | --------- |
| 7          | U+0000    | U+007F     | 0xxx xxxx | -         | -         | -         | -         | -         |
| 11         | U+0080    | U+07FF     | 110x xxxx | 10xx xxxx | -         | -         | -         | -         |
| 16         | U+0800    | U+FFFF     | 1110 xxxx | 10xx xxxx | 10xx xxxx | -         | -         | -         |
| 21         | U+10000   | U+1FFFFF   | 1111 0xxx | 10xx xxxx | 10xx xxxx | 10xx xxxx | -         | -         |
| 26         | U+200000  | U+3FFFFFF  | 1111 10xx | 10xx xxxx | 10xx xxxx | 10xx xxxx | 10xx xxxx | -         |
| 31         | U+4000000 | U+7FFFFFFF | 1111 110x | 10xx xxxx | 10xx xxxx | 10xx xxxx | 10xx xxxx | 10xx xxxx |

基本多文种平面之外字符，使用 4 字节形式编码。这些字符很多时候无法直接显示在文本编辑器里。如𢆥。有时候也会被用于区分错误的字符。

我们分别看三个从一个字节到三个字节的 UTF-8 编码例子：

| 实际字符 | 在 Unicode 字库序号的十六进制 | 在 Unicode 字库序号的二进制 | UTF-8 编码后的二进制          | UTF-8 编码后的十六进制 |
| -------- | ----------------------------- | --------------------------- | ----------------------------- | ---------------------- |
| $        | 0024                          | 010 0100                    | 0010 0100                     | 24                     |
| ¢        | 00A2                          | 000 1010 0010               | 1100 0010 1010 0010           | C2 A2                  |
| €        | 20AC                          | 0010 0000 1010 1100         | 1110 0010 1000 0010 1010 1100 | E2 82 AC               |

在文本编辑器中，一般会使用\u 将一个十六进制数字转换为 Unicode 字库序号，进而识别出 Unicode 对应的字符。如$可以表示为\u0024
UTF-8 的优缺点，以及其他更详细的描述，可以参考 Wikipedia: UTF-8

## UTF-16

如果字符编码 U 小于 0x10000，也就是十进制的 0 到 65535 之内，则直接使用两字节表示；
如果字符编码 U 大于 0x10000，由于 UNICODE 编码范围最大为 0x10FFFF，从 0x10000 到 0x10FFFF 之间共有 0xFFFFF 个编码，也就是需要 20 个 bit 就可以标示这些编码。用 U'表示从 0-0xFFFFF 之间的值，将其前 10 bit 作为高位和 16 bit 的数值 0xD800 进行 逻辑 or 操作，将后 10 bit 作为低位和 0xDC00 做 逻辑 or 操作，这样组成的 4 个 byte 就构成了 U 的编码。
由于一开始的 Unicode 只需要两个字节，所以 UTF-16 虽然也是变长编码方式，但是在最初却可以当做定长编码方式使用。UTF-16 每个字符都直接使用两个字节存储，所以就有字节顺序的问题，同一字节流可能会被解释为不同内容。如某字符为十六进制编码 4E59，按两个字节拆分为 4E 和 59，在 Mac 中和 Windows 中会解析如下：

- 读取顺序 显示字符
  Windows 4E 59 奎
  Mac 59 4E 乙
  在 Mac 上从低字节开始和在 Windows 上从高字节开始读取显示不同，从而导致在同一编码下的乱码问题。为了解决这个问题便引入了字节顺序标记（英语：byte-order mark，BOM）来标记是大端序还是小端序。

对于辅助平面的字符，由于超过了一个 16 位可以表示的长度，所以需要两个 16 位来表示。处于前面的 16 位被称为前导，而后面的被称为后缀。所以 UTF-16 要么是 2 字节，要么是 4 字节。

如何获取前导和后缀？基本多文种平面有一段代理区，不代表任何字符，通过对代理区的计算，高 10 位加上 0xD800 就是前导，低 10 位加上 0xDC00 就是后缀。前导和后尾组成的代理对表示 SP 里的一个码位。

很多人误以为 UTF-16 在早期是定长编码，其实它一开始就是变长的，同时期真正的二字节定长编码是 UCS-2。

## BOM

字节顺序标记（英语：byte-order mark，BOM）是一个有特殊含义的统一码字符，码点为 U+FEFF。当以 UTF-16 或 UTF-32 来将 UCS/统一码字符所组成的字符串编码时，这个字符被用来标示其字节序。经常被用于区分是否为 UTF 编码。

字符 U+FEFF 如果出现在字节流的开头，则用来标识该字节流的字节序，是高位在前还是低位在前。如果它出现在字节流的中间，则表达零宽度非换行空格的意义，用户看起来就是一个空格。从 Unicode3.2 开始，U+FEFF 只能出现在字节流的开头，只能用于标识字节序，就如它的名称——字节序标记——所表示的一样；除此以外的用法已被舍弃。取而代之的是，使用 U+2060 来表达零宽度无断空白。

UTF-8 以字节为编码单元，没有字节序的问题。但是某些操作系统也会使用带 BOM 的 UTF-8，叫做 UTF-8 with BOM。Python 中叫 utf-8-sig。Unicode 规范中说明 UTF-8 不必也不推荐使用 BOM。多数时候 UTF-8 都是不带 BOM 的，但是微软公司的某些软件（如 Excel）打开某些不带 BOM 的 utf8 文件（如 cvs 文件）会乱码，需要转换成带 BOM 的 utf8 编码才能正常显示。

所以 Java 中获取以 UTF-16 编码的字符串字节个数时，总是会比实际含有字符的字节个数多 2。不过目前已经有很多主流的文本编辑器支持不带 BOM 的 UTF 编码了，通过后缀（LE 和 BE）区分是小端还是大端。

详见 Wikipedia: 字节顺序标记。

"Use of BOM is neither required nor recommended for UTF-8" ( Unicode 5.0.0 Chapter 2.6)
更详细的区别请查阅知乎：「带 BOM 的 UTF-8」和「无 BOM 的 UTF-8」有什么区别？

IDEA 的 UTF-8 没有 BOM，但是如果打开（解码）某个已经存在的文件，该文件使用带 BOM 的 UTF-8 编码，BOM 会被忽略，但依然会保留。

另外，维基百科中阐述 UTF-8 和 UTF-16 属于五层模型的字符编码表（CEF）。但是个人理解，由于 BOM 的加入，实际上也包含了字符编码方案（CES）一层。所以 UTF-8 和 UTF-16 的实现实际包含了 CEF 和 CES 两个层次。UTF-8 和 UTF-16 也可以说是基于 Unicode 的字符编码。但和 ASCII 和 GB 2312 等字符编码不同的是，前者使用通用字符集作为其 ACR，而后者的 ACR 是自身规定（可能不通用）的。

知乎专栏: 字符编码的那些事对字符编码的阐述非常易懂，值得一看。

## 该使用什么编码？

非 Unicode 编码转换不当会造成各种乱码问题。那么具体应该如何选用合适的编码？

## 存储容量

先说 UTF-16，由于每个码位都使用 2 到 4 个字节来存储，对于含有大量中文或者其他二字节长的字符流来说，UTF-16 可以节省大量的存储空间。因为 UTF-16 并不需要像 UTF-8 那样通过牺牲很多标记位来标识一个字节表示的是什么，它只需一个字符来表示是大端序和小端序。

但是对于有大量西文字符的字符流来说 UTF-8 的优势就变得十分明显：UTF-8 只需要一个字节就能存储西文字符，这是 UTF-16 做不到的。所以在混合存储，或者是源代码、字节码文件等大量西文字符的文件，更倾向于 UTF-8。

UTF-8 存储中文比 UTF-16 要多出 50%，不推荐要大量显示中文的程序使用。—— 知乎轮子哥

而由于 UTF-8 的兼容性和对西文的支持，所以西方都提倡统一使用 UTF-8 作为字符编码，这样也的确可以彻底根除乱码问题。目前基本上所有的开发环境和源代码文件也基本上是统一 UTF-8。

但是统一使用 UTF-8 真的就没问题了吗？不是的。

国内网站也曾经掀起过一阵子 UTF-8 的热潮，小网站倒也没什么，但几个大型网站很快发现改用 UTF-8 之后流量费刷刷刷地往上涨，因为同样一个汉字在 GB2312 里只有 2 字节，单在 UTF-8 里变成了 3 字节，流量增加 50%，对于展示大量中文内容的网站来说简直就是灾难，即使使用所以过了没多久大网站们纷纷打定主意坚守 GB 系编码。对于需要数据库需要存储大量中文的网站，例如淘宝、CSDN 等博客网站，不合适的编码方式在流量峰值时会造成不小的流量开销，必须是一个值得考虑的问题。

## 存储效率

这里只从 UTF-8 和 UTF-16 两个编码来简单阐述下效率问题。

因为每个字符使用不同数量的字节编码，所以 UTF-8 编码的字符串，寻找串中第 N 个字符是一个 O(N)复杂度的操作。即串越长，则需要更多的时间来定位特定的字符。同时，还需要位变换来把字符编码成字节，把字节解码成字符。

而从 UTF-16 编码规则来看，仅仅将字符的高位和地位进行拆分变成两个字节。规则非常简单，编码效率很高，单字节 O(1)的查找效率也非常好。

所以选什么编码不是一句话、一篇文章可以决定的，必须通过一些流量测试和考量。目前大型的网站一般都不会只用一种单一的编码，而是多种编码混用，配合缓存和数据库的表压缩来减小流量压力。

不过值得一提的是，这种时间效率问题正在随着内存和 CPU 的发展而减小，现在已经不会作为主要考虑的问题了。

更多的讨论参考知乎：编程语言的字符编码选择 UTF-8 和 UTF-16 的优缺点

## Java 中的编码

由于 Java 高级语言的特性，如对字符串的封装、char、运行时 VM 环境等对底层的多种封装，Java 的编码也有很多值得详谈的地方。以上所有都是字符及编码的基础，下面来结合 Java 分析 Java 中的编码。

## Java 外部的编码

Java 运行时环境和外部环境使用的编码是不一样的。外部环境的编码可以使用 Charset.defaultcharset()获取。如果没有指定外部环境编码，就是操作系统的默认编码。jvm 操作 I/O 流时，如果不指定编码，也会使用这个编码，可以在启动 Java 时使用-Dfile.encoding=xxx 设置。通过 System.setProperty("file.encoding","GBK")能修改这个值，但由于 jvm 一旦启动就不能修改 jvm 默认字符集，所以修改这个值并没有什么卵用。

file.encoding 参数需要和 sun.jnu.encoding 作区分，后者主要设置的是下面三个地方的编码:

命令行参数
主类名称
环境变量
关于 Java 外部使用的编码，深入分析 Java 中的中文编码问题已经说的非常详细，所以本文在这里只讲述 JavaVM 内部的编码。

Java 内部的编码体系大致如图：

Java 编码体系
可以看到 Java 运行时主要的两个编码就是 UTF-8 和 UTF-16，而编译的开始，就要将各种不同编码的源代码文件的转码成 UTF-8。

其实不是 UTF-8，是一种 modified UTF-8，这里姑且先这么称呼。

## 编译时的编码转换

众所周知，Java 的源文件可以是任意的编码，但是在编译的时候，Javac 编译器默认会使用操作系统平台的编码解析字符，如果 Java 源文件是 UTF-8 编码的话，会造成乱码并拒绝编译：

Javac 拒绝编译
要想正确编译，需要使用-encoding 指定输入的 Java 源码文件的编码：

-encoding encodingSet the source file encoding name, such as EUC-JP and UTF-8. If -encoding is not specified, the platform default converter is used.

Javac 默认是使用操作系统平台的编码进行编译，在简体中文的 Windows 上，平台默认编码会是 GBK，那么 javac 就会默认假定输入的 Java 源码文件是以 GBK 编码的。

如果要想正确将源文件编译，即从橙色的源码文件到蓝色的编译器之间的箭头上，需要一个“桥梁”，而这个“桥梁”就是这个-encoding 参数。通过这个参数 javac 能够正确读取文件内容并将其中的字符串以 UTF-8 输出到 Class 文件里，就跟自己写个程序以 GBK 读文件以 UTF-8 写文件一样。

当编译期正确解码源代码字符后进行编译操作，生成 token 和抽象语法树，这时候就不再直接对源代码文件的字符进行操作了，编译器已经将其编码为 modified UTF-8 的字节流并对字节流进行操作。导致乱码的不是 Java 源码编译器的“编码”（写出 UTF-8）的过程，而是“解码”（读入 Java 源码内容）的过程。

JVM 规范中提到的 modified UTF-8: Chapter 4. The class File FormatString content is encoded in modified UTF-8. Modified UTF-8 strings are encoded so that code point sequences that contain only non-null ASCII characters can be represented using only 1 byte per code point, but all code points in the Unicode codespace can be represented. Modified UTF-8 strings are not null-terminated. The encoding is as follows: ...

## 运行时数据中的 UTF-16

JVM 中运行时数据都是使用 UTF-16 进行编码的。可能有个疑问，既然 UTF-8 兼容性那么好，为何不统一使用 UTF-16，而使用 UTF-8？

于是又要开始讲历史了。在 Unicode 最初诞生的时候，由于当时只有一个 16 位长的基本多文种平面，也就是只有 0~65535 的空间，两个字节刚好够用。所以 UTF-16 相比 UTF-8 来说也是有很多优势的。当时很多比较主流的 OS 或者 VM 都是使用 UTF-16 作为默认字符编码，例如 Windows NT 和 Java VM 的 runtime data，这也解释了为什么 Java 中 char 是两个字节。

Windows 中的 Unicode 实际上代表 UTF-16 LE，Unicode 并不是一种编码方式。如果还不理解 Unicode 是什么，就把它当成一个协议，而 UTF-XX 是对协议的一种实现吧 ..

但是到了 2001 年，中国人大举入侵 ISO 和 Unicode 委员会，用已经颁布的 GB18030-2000 为基础，在 Unicode 3.1 标准中一口气加入了 42711 个 CJK 扩展字符，整个 Unicode 字符集一下增大到 94205 个字符，2 个字节放不下了，UTF-16 原来是变长编码的事也被人想起来了（中国人偷笑，GB 系列从第一天起就是变长编码）。从此 UTF-16 就变得很尴尬，它一来存储空间利用率不高，二来又是个变长编码无法直接访问其中的码元。但是完全放弃 UTF-16 成本太高，所以现在 JVM 的运行时数据依然是 UTF16 编码的。

由于成本问题不能放弃 UTF-16，但是 UTF-8 的兼容性和流行程度，又使得 JVM 必须做点什么来使得其内部数据不会被编码方式影响，于是就有了这个 modified UTF-8。

那么 modified UTF-8 究竟是什么？

## modified UTF-8

在通常用法下，Java 程序语言在通过 InputStreamReader 和 OutputStreamWriter 读取和写入串的时候支持标准 UTF-8。在 Java 内部，以及 Class 文件里存储的字符串是以一种叫 modified UTF-8 的格式存储的。DataInput 和 DataOutput 的实现类也使用这种稍作修改的 UTF-8 来编码 Unicode 字符串，在使用中一般不会获取到 modified UTF-8 编码的字符串，但也有例外，例如 readUTF 方法。

String.getBytes("UTF-8") //拿到的字符串的标准 UTF-8 编码的字节数组
new String(bytes, "UTF-8") //是使用标准 UTF-8 的字节流构造 String.
modified UTF-8 大致和 UTF-8 编码相同，但是有以下三个不同点：

空字符（null character，U+0000）使用双字节的 0xc0 0x80，而不是单字节的 0x00。
仅使用 1 字节，2 字节和 3 字节格式，而 UTF-8 支持更多的字节
基本多文种平面之外的补充字符以代理对（surrogate pairs）的形式表示
使用双字节空字符保证了在已编码字符串中没有嵌入空字节。因为 C 语言等语言程序中，单字节空字符是用来标志字符串结尾的。当已编码字符串放到这样的语言中处理，一个嵌入的空字符将把字符串一刀两断。

modified UTF-8 在没有超过前三个字节表示的时候，和 UTF-8 编码方式一样，但是超过以后会以代理对（surrogate pairs）的形式表示。至于为什么要以代理对形式表示。这个是因为 JVM 的默认编码是 UTF-16 导致的。

一开始的 Unicode 只有一个可以用 16 位长完全表示的基本多文种平面，所以 Java 中的字符（char）为 16 位长，一个 char 可以存所有的字符。后来 Unicode 增加了很多辅助平面，两个字节存不下那些字符，但是为了向后兼容 Java 不可能更改它的基本语法实现，于是对于超过 U+FFFF 的字符 （就是所谓的扩展字符）就需要用两个 16 位长数据来表示，modified UTF-8 由 UTF-16 格式的代理码元来代替原先的 Unicode 码元作为字符编码表的码元。每个代理码单元由 3 个字节（就是一个 modified UTF-8 编码出来的最大字节长）表示。所以在 Java 内部数据是统一使用 modified UTF-8 进行编码的，这个编码解码出来的码元是 UTF-16 编码出来的 2 字节。JVM 把 UTF-16 编码出来的 16 位长的数据（2 字节，操作系统用 8 位长的数据，即 1 字节）作为最小单位进行信息交换。这样的话既不改变原来 JVM 中的编码规则，又减少了很多扩展字符从 UTF-8 转码到 UTF-16 时的运算量，是不是很刺激？

modified UTF-8 保证了一个已编码字符串可以一次编为一个 UTF-16 码，而不是一次一个 Unicode 码位，使得所有的 Unicode 字符都能在 Java 上显示。

不过也不是没有缺点的，使用 modified UTF-8 进行解码解出来的是 UTF-16 编码编出来的数据，而 UTF-16 处理扩展字符需要两个 16 位长表示。也就是说，要用两个代理码元共同表示一个 Unicode 码位。原本使用 UTF-8 编码只需要最多 4 个字节就能存储一个 Unicode 码位，使用 modified UTF-8 编码后却需要 6 个字节来存储两个代码单元。

总结起来就是，modified UTF-8 是对 UTF-16 的再编码，modified UTF-8 和 UTF-8 是两种完全不同的编码。

所以 JVM 无需解码 UTF-16 的数据，modified UTF-8 代理码元会处理这个映射关系。

## 文末

个人整理、理解，错漏之处在所难免，还望指教。
